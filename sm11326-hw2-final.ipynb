{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d03af9-d433-4d41-a766-bcbda55253ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/23 01:00:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-sm11326:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code><my-app-name></code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=<my-app-name>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"<my-app-name>\")\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52cd742-faff-4097-a6e6-1a2003f036b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+\n",
      "|      Date|    Time|Transaction|         Item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|10:07:57|          3|          Jam|\n",
      "|2016-10-30|10:07:57|          3|      Cookies|\n",
      "|2016-10-30|10:08:41|          4|       Muffin|\n",
      "|2016-10-30|10:13:03|          5|       Coffee|\n",
      "|2016-10-30|10:13:03|          5|       Pastry|\n",
      "|2016-10-30|10:13:03|          5|        Bread|\n",
      "|2016-10-30|10:16:55|          6|    Medialuna|\n",
      "|2016-10-30|10:16:55|          6|       Pastry|\n",
      "|2016-10-30|10:16:55|          6|       Muffin|\n",
      "|2016-10-30|10:19:12|          7|    Medialuna|\n",
      "|2016-10-30|10:19:12|          7|       Pastry|\n",
      "|2016-10-30|10:19:12|          7|       Coffee|\n",
      "|2016-10-30|10:19:12|          7|          Tea|\n",
      "|2016-10-30|10:20:51|          8|       Pastry|\n",
      "|2016-10-30|10:20:51|          8|        Bread|\n",
      "|2016-10-30|10:21:59|          9|        Bread|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUESTION 1 :  25 Points, Data : \"shared/data/Bakert.csv\" \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession(sc)\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"shared/data/Bakery.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6778719b-0f60-4bed-86df-e72ff71c9cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+-------+----+\n",
      "|      Date|    Time|Transaction|         Item|weekday|Hour|\n",
      "+----------+--------+-----------+-------------+-------+----+\n",
      "|2016-10-30|09:58:11|          1|        Bread| Sunday|   9|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian| Sunday|  10|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian| Sunday|  10|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate| Sunday|  10|\n",
      "|2016-10-30|10:07:57|          3|          Jam| Sunday|  10|\n",
      "|2016-10-30|10:07:57|          3|      Cookies| Sunday|  10|\n",
      "|2016-10-30|10:08:41|          4|       Muffin| Sunday|  10|\n",
      "|2016-10-30|10:13:03|          5|       Coffee| Sunday|  10|\n",
      "|2016-10-30|10:13:03|          5|       Pastry| Sunday|  10|\n",
      "|2016-10-30|10:13:03|          5|        Bread| Sunday|  10|\n",
      "|2016-10-30|10:16:55|          6|    Medialuna| Sunday|  10|\n",
      "|2016-10-30|10:16:55|          6|       Pastry| Sunday|  10|\n",
      "|2016-10-30|10:16:55|          6|       Muffin| Sunday|  10|\n",
      "|2016-10-30|10:19:12|          7|    Medialuna| Sunday|  10|\n",
      "|2016-10-30|10:19:12|          7|       Pastry| Sunday|  10|\n",
      "|2016-10-30|10:19:12|          7|       Coffee| Sunday|  10|\n",
      "|2016-10-30|10:19:12|          7|          Tea| Sunday|  10|\n",
      "|2016-10-30|10:20:51|          8|       Pastry| Sunday|  10|\n",
      "|2016-10-30|10:20:51|          8|        Bread| Sunday|  10|\n",
      "|2016-10-30|10:21:59|          9|        Bread| Sunday|  10|\n",
      "+----------+--------+-----------+-------------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------+-----------+---------+-------+----+\n",
      "|      Date|    Time|Transaction|     Item|weekday|Hour|\n",
      "+----------+--------+-----------+---------+-------+----+\n",
      "|2016-10-31|08:28:31|         81|   Coffee| Monday|   8|\n",
      "|2016-10-31|08:28:31|         81|     Cake| Monday|   8|\n",
      "|2016-10-31|08:47:05|         82|  Tartine| Monday|   8|\n",
      "|2016-10-31|08:47:05|         82|     NONE| Monday|   8|\n",
      "|2016-10-31|08:47:05|         82|    Bread| Monday|   8|\n",
      "|2016-10-31|08:57:47|         83|   Coffee| Monday|   8|\n",
      "|2016-10-31|08:57:47|         83|    Bread| Monday|   8|\n",
      "|2016-10-31|09:04:06|         84|    Bread| Monday|   9|\n",
      "|2016-10-31|09:10:09|         85|   Coffee| Monday|   9|\n",
      "|2016-10-31|09:10:09|         85|   Coffee| Monday|   9|\n",
      "|2016-10-31|09:10:09|         85|   Pastry| Monday|   9|\n",
      "|2016-10-31|09:10:09|         85|Medialuna| Monday|   9|\n",
      "|2016-10-31|09:11:08|         86|    Juice| Monday|   9|\n",
      "|2016-10-31|09:12:56|         87|    Bread| Monday|   9|\n",
      "|2016-10-31|09:16:01|         88|   Coffee| Monday|   9|\n",
      "|2016-10-31|09:16:01|         88|   Coffee| Monday|   9|\n",
      "|2016-10-31|09:16:01|         88|      Jam| Monday|   9|\n",
      "|2016-10-31|09:19:41|         89|    Bread| Monday|   9|\n",
      "|2016-10-31|09:20:12|         90|    Bread| Monday|   9|\n",
      "|2016-10-31|09:20:12|         90|   Coffee| Monday|   9|\n",
      "+----------+--------+-----------+---------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, hour\n",
    "df = df.withColumn('Date', col('Date').cast('date'))\n",
    "df = df.withColumn('weekday', date_format(col('Date'), 'EEEE'))\n",
    "df = df.withColumn('Hour', hour(col('Time')))\n",
    "df.show()\n",
    "df_monday = df.filter(col('weekday') == 'Monday')\n",
    "df_monday.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5e530e-8c9d-49d5-a887-3243c7738604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+--------------+\n",
      "|Date      |Hour|Item  |total_quantity|\n",
      "+----------+----+------+--------------+\n",
      "|2016-10-31|8   |Coffee|2             |\n",
      "|2016-10-31|9   |Coffee|11            |\n",
      "|2016-10-31|10  |Coffee|10            |\n",
      "|2016-10-31|11  |Coffee|13            |\n",
      "|2016-11-07|8   |Pastry|1             |\n",
      "|2016-11-07|9   |Bread |3             |\n",
      "|2016-11-07|10  |Coffee|7             |\n",
      "|2016-11-07|11  |Coffee|10            |\n",
      "|2016-11-14|7   |Coffee|1             |\n",
      "|2016-11-14|8   |Coffee|2             |\n",
      "|2016-11-14|9   |Coffee|5             |\n",
      "|2016-11-14|10  |Coffee|5             |\n",
      "|2016-11-14|11  |Bread |5             |\n",
      "|2016-11-21|7   |Coffee|1             |\n",
      "|2016-11-21|8   |Coffee|2             |\n",
      "|2016-11-21|9   |Coffee|8             |\n",
      "|2016-11-21|10  |Coffee|4             |\n",
      "|2016-11-21|11  |Coffee|4             |\n",
      "|2016-11-28|7   |Coffee|1             |\n",
      "|2016-11-28|8   |Coffee|1             |\n",
      "+----------+----+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df_monday_filtered = df_monday.filter((col('Hour') >= 7) & (col('Hour') <= 11))\n",
    "df_grouped = df_monday_filtered.groupBy(\"Date\", \"Hour\", \"Item\").agg(count(\"Item\").alias(\"total_quantity\"))\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec = Window.partitionBy(\"Date\", \"Hour\").orderBy(col(\"total_quantity\").desc())\n",
    "df_with_row_num = df_grouped.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "df_top_items = df_with_row_num.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "df_top_items.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3f1d77-698c-4105-bf7e-2bbc34f54912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+\n",
      "|DayType|Daypart  |Top_2_Items  |\n",
      "+-------+---------+-------------+\n",
      "|Weekday|Breakfast|Coffee, Bread|\n",
      "|Weekday|Dinner   |Coffee, Bread|\n",
      "|Weekday|Lunch    |Coffee, Bread|\n",
      "|Weekend|Breakfast|Coffee, Bread|\n",
      "|Weekend|Dinner   |Coffee, Bread|\n",
      "|Weekend|Lunch    |Coffee, Bread|\n",
      "+-------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# QUESTION 2 : 25 Points, Data = \"shared/data/Bakery.csv\"\n",
    "\n",
    "from pyspark.sql.functions import col, when, row_number, count, collect_list, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = df.withColumn(\"DayType\", when(col(\"weekday\").isin(\"Saturday\", \"Sunday\"), \"Weekend\").otherwise(\"Weekday\"))\n",
    "\n",
    "df = df.withColumn(\"Daypart\", when((col(\"hour\") >= 6) & (col(\"hour\") < 11), \"Breakfast\")\n",
    "                                .when((col(\"hour\") >= 11) & (col(\"hour\") < 16), \"Lunch\")\n",
    "                                .otherwise(\"Dinner\"))\n",
    "grouped_df = df.groupBy(\"DayType\", \"Daypart\", \"Item\").agg(count(\"Item\").alias(\"total_qty\"))\n",
    "window_spec = Window.partitionBy(\"DayType\", \"Daypart\").orderBy(col(\"total_qty\").desc())\n",
    "\n",
    "ranked_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                      .filter(col(\"rank\") <= 2) \\\n",
    "                      .drop(\"rank\")\n",
    "result = ranked_df.groupBy(\"DayType\", \"Daypart\").agg(concat_ws(\", \", collect_list(\"Item\")).alias(\"Top_2_Items\"))\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b7e847-701c-418b-88f0-be1572c0b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|rpt_area_desc        |count|\n",
      "+---------------------+-----+\n",
      "|Bed&Breakfast Home   |4    |\n",
      "|Summer Camps         |4    |\n",
      "|Institutions         |30   |\n",
      "|Local Confinement    |2    |\n",
      "|Mobile Food          |147  |\n",
      "|School Buildings     |89   |\n",
      "|Summer Food          |242  |\n",
      "|Swimming Pools       |420  |\n",
      "|Day Care             |173  |\n",
      "|Tattoo Establishments|36   |\n",
      "|Residential Care     |154  |\n",
      "|Bed&Breakfast Inn    |2    |\n",
      "|Adult Day Care       |5    |\n",
      "|Lodging              |62   |\n",
      "|Food Service         |1093 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# QUESTION 3  20 Points , Data = \"shared/data/Restaurants_in_Durham_County_NC.json\"\n",
    "\n",
    "df = spark.read.json(\"shared/data/Restaurants_in_Durham_County_NC.json\")\n",
    "grouped_df = df.groupBy(\"fields.rpt_area_desc\").count()\n",
    "grouped_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3acdc032-0981-4a2f-a74a-72bc1212db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Arab Emirates, 76.28%\n",
      "Montserrat, -63.19%\n",
      "Row(Country Name='United Arab Emirates', 1990='1.82625', 2000='3.2193', Percentage Change=76.27926078028749)\n",
      "Row(Country Name='Montserrat', 1990='0.01073', 2000='0.00395', Percentage Change=-63.18732525629077)\n"
     ]
    }
   ],
   "source": [
    "#QUESTION 4 20 Points, Data = \"shared/data/populationbycountry19802010millions.csv\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "file_path = \"shared/data/populationbycountry19802010millions.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "cleaned_data = data.toDF(*(c.strip() for c in data.columns))\n",
    "population_data = cleaned_data.select(\"_c0\", \"1990\", \"2000\").na.drop()\n",
    "\n",
    "population_data = population_data.withColumnRenamed(\"_c0\", \"Country Name\")\n",
    "population_change = population_data.withColumn(\n",
    "    \"Percentage Change\",\n",
    "    ((F.col(\"2000\") - F.col(\"1990\")) / F.col(\"1990\")) * 100\n",
    ").filter(F.col(\"Percentage Change\").isNotNull())\n",
    "\n",
    "max_increase = population_change.orderBy(F.col(\"Percentage Change\").desc()).first()\n",
    "max_decrease = population_change.orderBy(F.col(\"Percentage Change\").asc()).first()\n",
    "print(f\"{max_increase['Country Name']}, {max_increase['Percentage Change']:.2f}%\")\n",
    "print(f\"{max_decrease['Country Name']}, {max_decrease['Percentage Change']:.2f}%\")\n",
    "print(max_increase)\n",
    "print(max_decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926f95fd-fb69-4606-a11b-9deed4cbd7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word|count |\n",
      "+----+------+\n",
      "|the |142965|\n",
      "|to  |87873 |\n",
      "|p   |78583 |\n",
      "|of  |75074 |\n",
      "|and |70933 |\n",
      "|in  |52844 |\n",
      "|a   |50187 |\n",
      "|for |28369 |\n",
      "|he  |27781 |\n",
      "|is  |27646 |\n",
      "|that|27443 |\n",
      "|s   |25354 |\n",
      "|on  |23636 |\n",
      "|are |19529 |\n",
      "|with|18699 |\n",
      "|be  |17764 |\n",
      "|as  |16110 |\n",
      "|have|16083 |\n",
      "|at  |15209 |\n",
      "|said|14893 |\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 5 20 POINTS, DATA : HW1 TEXT - WQRD COUNT\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "file_paths = [\"20-01.txt\", \"20-02.txt\", \"20-03.txt\", \"20-04.txt\", \"20-05.txt\"]\n",
    "global_word_count = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    data = spark.read.text(file_path)\n",
    "    normalized_data = data.select(F.lower(F.regexp_replace(\"value\", \"[^0-9a-z]\", \" \")).alias(\"normalized_text\"))\n",
    "    tokenizer = RegexTokenizer(inputCol=\"normalized_text\", outputCol=\"words\", pattern=\"\\\\s+\")\n",
    "    pipeline = Pipeline(stages=[tokenizer])\n",
    "    model = pipeline.fit(normalized_data)\n",
    "    tokenized_data = model.transform(normalized_data)\n",
    "    exploded_data = tokenized_data.select(F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "    word_count = exploded_data.groupBy(\"word\").count()\n",
    "    for row in word_count.collect():\n",
    "        word = row['word']\n",
    "        count = row['count']\n",
    "        if word in global_word_count:\n",
    "            global_word_count[word] += count\n",
    "        else:\n",
    "            global_word_count[word] = count\n",
    "\n",
    "global_word_count_df = spark.createDataFrame(global_word_count.items(), [\"word\", \"count\"])\n",
    "global_word_count_df = global_word_count_df.orderBy(F.desc(\"count\"))\n",
    "global_word_count_df.show(truncate=False)\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df89aedb-5883-4302-a0c0-df1ddbd4b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the : 17436\n",
      "in the : 12777\n",
      "covid 19 : 8726\n",
      "to the : 8331\n",
      "for the : 5573\n",
      "n t : 5393\n",
      "on the : 4990\n",
      "to be : 4571\n",
      "will be : 4175\n",
      "and the : 4026\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 6 : 20 POINTS, DATA = HW 1 - TOP 10 MOST COMMON BIGRAMS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bigram Count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "files = [\"20-01.txt\", \"20-02.txt\", \"20-03.txt\", \"20-04.txt\", \"20-05.txt\"]\n",
    "global_bigram_counts = {}\n",
    "\n",
    "for file_path in files:\n",
    "    text_data = spark.read.text(file_path)\n",
    "    cleaned_data = text_data.select(\n",
    "        F.lower(F.regexp_replace('value', '[^0-9A-Za-z]', ' ')).alias('cleaned_text')\n",
    "    )\n",
    "\n",
    "    tokenized_data = cleaned_data.select(\n",
    "        F.split(F.col('cleaned_text'), ' ').alias('words')\n",
    "    )\n",
    "\n",
    "    bigram = NGram(n=2, inputCol='words', outputCol='bigrams')\n",
    "    bigram_data = bigram.transform(tokenized_data)\n",
    "    exploded_bigrams = bigram_data.select(F.explode('bigrams').alias('bigram'))\n",
    "    filtered_bigrams = exploded_bigrams.filter(F.col('bigram').rlike('^[a-z0-9]+ [a-z0-9]+$'))\n",
    "\n",
    "    bigram_counts = filtered_bigrams.groupBy('bigram').count()\n",
    "    \n",
    "    for row in bigram_counts.collect():\n",
    "        bigram = row['bigram']\n",
    "        count = row['count']\n",
    "        if bigram in global_bigram_counts:\n",
    "            global_bigram_counts[bigram] += count\n",
    "        else:\n",
    "            global_bigram_counts[bigram] = count\n",
    "\n",
    "global_top_bigrams = sorted(global_bigram_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "formatted_output = [f\"{bigram} : {count}\" for bigram, count in global_top_bigrams]\n",
    "\n",
    "for result in formatted_output:\n",
    "    print(result)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3183be5-1936-4832-93fb-a17c3a5b43f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Answer: \n",
      "\n",
      "\n",
      "Restaurant Name: OLD HAVANA SANDWICH SHOP\n",
      "Geolocation: Latitude 35.9932826, Longitude -78.8981331\n",
      "Distance from Reference Point: 0.13 miles\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Latitude is 35.9932826 and the Longitude is -78.8981331\n",
      "Number of foreclosures within 1 mile of the closest restaurant: 320\n",
      "\n",
      "Distances to Foreclosures (VERIFICATION PRINTING IN ASCENDING ORDER):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------------+\n",
      "|geocode                  |distance_to_restaurant|\n",
      "+-------------------------+----------------------+\n",
      "|[35.995413, -78.8950321] |0.2274148959935075    |\n",
      "|[35.995797, -78.895396]  |0.2315019166859024    |\n",
      "|[35.9965639, -78.9004693]|0.2616405140108075    |\n",
      "|[35.9961544, -78.8929116]|0.3529462418852746    |\n",
      "|[35.9961544, -78.8929116]|0.3529462418852746    |\n",
      "|[35.995954, -78.892736]  |0.3536878179799853    |\n",
      "|[35.995954, -78.892736]  |0.3536878179799853    |\n",
      "|[35.9968123, -78.8935006]|0.3557226602418183    |\n",
      "|[35.9968123, -78.8935006]|0.3557226602418183    |\n",
      "|[35.9988695, -78.8990179]|0.389173850545146     |\n",
      "|[35.9988695, -78.8990179]|0.389173850545146     |\n",
      "|[35.9988695, -78.8990179]|0.389173850545146     |\n",
      "|[35.995823, -78.891795]  |0.3954043208699746    |\n",
      "|[35.995823, -78.891795]  |0.3954043208699746    |\n",
      "|[35.995823, -78.891795]  |0.3954043208699746    |\n",
      "|[35.995582, -78.8913248] |0.41242418898431704   |\n",
      "|[35.995582, -78.8913248] |0.41242418898431704   |\n",
      "|[35.988392, -78.893271]  |0.43366321292393134   |\n",
      "|[35.988392, -78.893271]  |0.43366321292393134   |\n",
      "|[35.995761, -78.8907861] |0.4449782627517967    |\n",
      "+-------------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 7 - BONUS QUESTION - 35 POINTS - HAVERSINE \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from haversine import haversine, Unit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Find Restaurants and Foreclosures\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "foreclosure_file = \"shared/data/durham-nc-foreclosure-2006-2016.json\"\n",
    "restaurants_file = \"shared/data/Restaurants_in_Durham_County_NC.json\"\n",
    "foreclosure_df = spark.read.json(foreclosure_file)\n",
    "restaurants_df = spark.read.json(restaurants_file)\n",
    "\n",
    "active_restaurants_df = restaurants_df.filter(\n",
    "    (restaurants_df.fields.status == \"ACTIVE\") & \n",
    "    (restaurants_df.fields.rpt_area_desc == \"Food Service\")\n",
    ")\n",
    "\n",
    "reference_point = (35.994914, -78.897133)\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    if lat1 is None or lon1 is None or lat2 is None or lon2 is None:\n",
    "        return None\n",
    "    return haversine((lat1, lon1), (lat2, lon2), unit=Unit.MILES)\n",
    "\n",
    "calculate_distance_udf = F.udf(calculate_distance, DoubleType())\n",
    "\n",
    "active_restaurants_with_distance_df = active_restaurants_df.withColumn(\n",
    "    \"distance\", \n",
    "    calculate_distance_udf(\n",
    "        F.lit(reference_point[0]), \n",
    "        F.lit(reference_point[1]), \n",
    "        active_restaurants_df.fields.geolocation[0], \n",
    "        active_restaurants_df.fields.geolocation[1]\n",
    "    )\n",
    ").filter(F.col(\"distance\").isNotNull())\n",
    "\n",
    "closest_restaurant_df = active_restaurants_with_distance_df.orderBy(\"distance\").limit(1)\n",
    "\n",
    "for row in closest_restaurant_df.collect():\n",
    "    restaurant_name = row['fields'].premise_name\n",
    "    geolocation = row['fields'].geolocation\n",
    "    distance = row['distance']\n",
    "    print(f\"Part 1 Answer: \\n\\n\")\n",
    "    print(f\"Restaurant Name: {restaurant_name}\")\n",
    "    print(f\"Geolocation: Latitude {geolocation[0]}, Longitude {geolocation[1]}\")\n",
    "    print(f\"Distance from Reference Point: {distance:.2f} miles\")\n",
    "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "closest_restaurant_coords = closest_restaurant_df.select(\n",
    "    active_restaurants_df.fields.geolocation[0].alias(\"latitude\"),\n",
    "    active_restaurants_df.fields.geolocation[1].alias(\"longitude\")\n",
    ").first()\n",
    "\n",
    "if closest_restaurant_coords:\n",
    "    latitude, longitude = closest_restaurant_coords\n",
    "    print(f\"The Latitude is {latitude} and the Longitude is {longitude}\")\n",
    "\n",
    "    foreclosures_with_distance_df = foreclosure_df.withColumn(\n",
    "        'distance_to_restaurant',\n",
    "        calculate_distance_udf(\n",
    "            F.lit(latitude), \n",
    "            F.lit(longitude), \n",
    "            foreclosure_df.fields.geocode[0], \n",
    "            foreclosure_df.fields.geocode[1]\n",
    "        )\n",
    "    ).filter(F.col('distance_to_restaurant').isNotNull())\n",
    "\n",
    "    foreclosures_within_radius_df = foreclosures_with_distance_df.filter(\n",
    "        F.col('distance_to_restaurant') <= 1.0\n",
    "    )\n",
    "\n",
    "    foreclosure_count = foreclosures_within_radius_df.count()\n",
    "    print(f\"Number of foreclosures within 1 mile of the closest restaurant: {foreclosure_count}\\n\")\n",
    "    print(\"Distances to Foreclosures (VERIFICATION PRINTING IN ASCENDING ORDER):\")\n",
    "    \n",
    "    foreclosures_with_distance_df.select(\n",
    "        'fields.geocode', \n",
    "        'distance_to_restaurant'\n",
    "    ).orderBy('distance_to_restaurant', ascending=True).show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
